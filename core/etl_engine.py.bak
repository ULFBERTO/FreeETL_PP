import polars as pl
from typing import Dict, Any, List, Optional, Tuple
import networkx as nx
from PyQt6.QtCore import QObject, pyqtSignal
import os
import pandas as pd
import json
import requests

class ETLEngine(QObject):
    # Señales
    execution_progress = pyqtSignal(str)  # Señal para informar del progreso
    execution_finished = pyqtSignal(bool, str)  # Señal para informar del resultado (éxito, mensaje)
    node_executed = pyqtSignal(int, object)  # Señal para informar que un nodo se ha ejecutado (id, dataframe)
    log_event = pyqtSignal(dict)
    
    def __init__(self):
        super().__init__()
        self.pipeline = nx.DiGraph()
        self.node_dataframes = {}  # Almacena los dataframes de cada nodo
        self._stop_requested = False  # Bandera para detener ejecución
        # Utilidad para timestamps
        try:
            from datetime import datetime as _dt
            self._dt = _dt
        except Exception:
            self._dt = None

    def _log(self, level: str, message: str, node_id: Optional[int] = None, stage: Optional[str] = None, error: Optional[str] = None, traceback_str: Optional[str] = None):
        """Emite evento de log estructurado y mantiene compatibilidad con execution_progress."""
        try:
            evt = {
                'timestamp': (self._dt.now().strftime('%Y-%m-%d %H:%M:%S') if self._dt else None),
                'level': (level or 'INFO').upper(),
                'message': str(message) if message is not None else '',
            }
            if node_id is not None:
                evt['node_id'] = int(node_id)
                try:
                    n = self.pipeline.nodes[node_id]
                    evt['node_type'] = n.get('type')
                    cfg = n.get('config') or {}
                    evt['node_subtype'] = cfg.get('subtype')
                except Exception:
                    pass
            if stage:
                evt['stage'] = stage
            if error:
                evt['error'] = str(error)
            if traceback_str:
                evt['traceback'] = str(traceback_str)
            self.log_event.emit(evt)
        except Exception:
            try:
                self.execution_progress.emit(f"[{level}] {message}")
            except Exception:
                pass

    def set_pipeline(self, pipeline: nx.DiGraph, node_configs: Dict[int, Dict[str, Any]]):
        """Establece el pipeline a partir del grafo visual y las configuraciones"""
        self.pipeline = pipeline.copy()
        for node_id, config in node_configs.items():
            if node_id in self.pipeline.nodes:
                self.pipeline.nodes[node_id]['config'] = config
                
    def add_source(self, node_id: int, config: Dict[str, Any]):
        """Add a source node to the pipeline"""
        self.pipeline.add_node(node_id, type='source', config=config)
        
    def add_transform(self, node_id: int, config: Dict[str, Any]):
        """Add a transform node to the pipeline"""
        self.pipeline.add_node(node_id, type='transform', config=config)
        
    def add_destination(self, node_id: int, config: Dict[str, Any]):
        """Add a destination node to the pipeline"""
        self.pipeline.add_node(node_id, type='destination', config=config)
        
    def add_connection(self, source_id: int, target_id: int):
        """Add a connection between nodes"""
        self.pipeline.add_edge(source_id, target_id)
        
    def execute_source(self, node_id: int) -> pl.DataFrame:
        """Ejecuta un nodo de origen y retorna un DataFrame de Polars."""
        self.execution_progress.emit(f"Ejecutando nodo de origen {node_id}...")
        try:
            self._log('INFO', 'Inicio de origen', node_id=node_id, stage='start')
        except Exception:
            pass
        config = self.pipeline.nodes[node_id]['config']
        subtype = config.get('subtype')

        # Usar datos precargados si existen
        if 'dataframe' in config and isinstance(config['dataframe'], (pl.DataFrame, pd.DataFrame)):
            self.execution_progress.emit(f"Usando datos precargados en nodo {node_id}")
            try:
                self._log('INFO', 'Usando datos precargados', node_id=node_id, stage='cached')
            except Exception:
                pass
            df = config['dataframe']
            if isinstance(df, pd.DataFrame):
                df = pl.from_pandas(df)
            res = self._apply_select_and_rename(df, config)
            try:
                self.execution_progress.emit(f"Nodo origen {node_id} columnas: {list(res.columns)}")
            except Exception:
                pass
            return res

        try:
            if subtype == 'csv':
                path = config.get('path')
                if not path:
                    raise ValueError(f"No se especificó ruta de archivo para el nodo {node_id}")
                df = pl.read_csv(path)
                res = self._apply_select_and_rename(df, config)
                try:
                    self.execution_progress.emit(f"Nodo origen {node_id} columnas: {list(res.columns)}")
                except Exception:
                    pass
                return res

            elif subtype == 'excel':
                path = config.get('path')
                if not path:
                    raise ValueError(f"No se especificó ruta de archivo para el nodo {node_id}")
                # pl.read_excel puede no estar disponible en todas las versiones; fallback a pandas
                try:
                    df = pl.read_excel(path)
                except Exception:
                    pdf = pd.read_excel(path)
                    df = pl.from_pandas(pdf)
                res = self._apply_select_and_rename(df, config)
                try:
                    self.execution_progress.emit(f"Nodo origen {node_id} columnas: {list(res.columns)}")
                except Exception:
                    pass
                return res

            elif subtype == 'json':
                path = config.get('path')
                if not path:
                    raise ValueError(f"No se especificó ruta de archivo para el nodo {node_id}")
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                # Detectar estructura
                if isinstance(data, list):
                    df = pl.DataFrame(data)
                elif isinstance(data, dict):
                    # Si tiene 'data' o similar, intentar usarlo
                    key = 'data' if 'data' in data else None
                    if key:
                        df = pl.DataFrame(data[key])
                    else:
                        df = pl.DataFrame([data])
                else:
                    raise ValueError("Estructura JSON no soportada para conversión a DataFrame")
                return self._apply_select_and_rename(df, config)

            elif subtype == 'parquet':
                path = config.get('path')
                if not path:
                    raise ValueError(f"No se especificó ruta de archivo para el nodo {node_id}")
                df = pl.read_parquet(path)
                res = self._apply_select_and_rename(df, config)
                try:
                    self.execution_progress.emit(f"Nodo origen {node_id} columnas: {list(res.columns)}")
                except Exception:
                    pass
                return res

            elif subtype == 'database':
                db_type = config.get('db_type')
                host = config.get('host')
                port = config.get('port')
                user = config.get('user')
                password = config.get('password')
                database = config.get('database')
                query = config.get('query')
                if not query:
                    raise ValueError("Debe especificar una consulta SQL en la configuración del nodo de base de datos")

                conn_str = self._build_connection_string(db_type, host, port, user, password, database)
                self.execution_progress.emit(f"Leyendo desde base de datos ({db_type})...")
                try:
                    if (db_type or '').lower() == 'mysql':
                        # Crear engine con (posible) SSL según config
                        engine = self._make_sqlalchemy_engine(db_type, conn_str, config)
                        pdf = pd.read_sql_query(query, engine)
                    else:
                        # Camino estándar
                        df = self._read_sql(conn_str, query)
                        res = self._apply_select_and_rename(df, config)
                        try:
                            self.execution_progress.emit(f"Nodo origen {node_id} columnas: {list(res.columns)}")
                        except Exception:
                            pass
                        return res
                except Exception as e:
                    # Reintentar con modo SSL opuesto si parece error de SSL y es MySQL
                    if (db_type or '').lower() == 'mysql' and self._should_retry_ssl(e, config):
                        try:
                            retry_mode = 'DISABLED' if self._was_ssl_enabled(config) else 'REQUIRED'
                            engine = self._make_sqlalchemy_engine(db_type, conn_str, config, ssl_mode_override=retry_mode)
                            pdf = pd.read_sql_query(query, engine)
                            self.execution_progress.emit(f"Reintento MySQL con SSL='{retry_mode}' exitoso")
                        except Exception:
                            self.execution_progress.emit(f"Fallo reintento MySQL cambiando SSL: {e}")
                            raise
                    else:
                        raise
                # Convertir a Polars y aplicar selección/renombrado
                df = pl.from_pandas(pdf)
                res = self._apply_select_and_rename(df, config)
                try:
                    self.execution_progress.emit(f"Nodo origen {node_id} columnas: {list(res.columns)}")
                except Exception:
                    pass
                return res

            elif subtype == 'api':
                url = config.get('url')
                method = (config.get('method') or 'GET').upper()
                headers = self._parse_kv_string(config.get('headers')) if isinstance(config.get('headers'), str) else config.get('headers')
                params = self._parse_kv_string(config.get('params')) if isinstance(config.get('params'), str) else config.get('params')
                if not url:
                    raise ValueError("Debe especificar una URL para el origen API")
                self.execution_progress.emit(f"Llamando API {method} {url}...")
                resp = requests.request(method, url, headers=headers, params=params, timeout=60)
                resp.raise_for_status()
                try:
                    data = resp.json()
                except Exception:
                    # Intentar CSV si el contenido lo parece
                    try:
                        from io import BytesIO
                        df = pl.read_csv(BytesIO(resp.content))
                        res = self._apply_select_and_rename(df, config)
                        try:
                            self.execution_progress.emit(f"Nodo origen {node_id} columnas: {list(res.columns)}")
                        except Exception:
                            pass
                        return res
                    except Exception:
                        raise ValueError("La respuesta de la API no es JSON ni CSV soportado")
                # Normalizar JSON
                if isinstance(data, list):
                    df = pl.DataFrame(data)
                elif isinstance(data, dict):
                    key = 'data' if 'data' in data else None
                    if key:
                        df = pl.DataFrame(data[key])
                    else:
                        df = pl.DataFrame([data])
                else:
                    raise ValueError("Estructura de respuesta de API no soportada")
                return self._apply_select_and_rename(df, config)

            else:
                raise ValueError(f"Tipo de origen desconocido o no soportado para nodo {node_id}")
        except Exception as e:
            self.execution_progress.emit(f"Error al ejecutar origen {node_id}: {e}")
            try:
                import traceback as _tb
                self._log('ERROR', 'Error al ejecutar origen', node_id=node_id, stage='error', error=str(e), traceback_str=_tb.format_exc())
            except Exception:
                pass
            raise
        
    def execute_transform(self, node_id: int, df: pl.DataFrame) -> pl.DataFrame:
        """Ejecuta un nodo de transformación sobre el DataFrame de entrada."""
        self.execution_progress.emit(f"Ejecutando transformación en nodo {node_id}...")
        config = self.pipeline.nodes[node_id]['config']
        subtype = config.get('subtype')

        result_df = df
        try:
            if subtype == 'filter':
                # Modo nuevo: reglas estructuradas
                rules = config.get('filter_rules')
                mode = (config.get('filter_mode') or 'all').lower()  # 'all' (AND) o 'any' (OR)
                if isinstance(rules, list) and rules:
                    try:
                        result_df = self._execute_filter_rules(df, rules, mode)
                    except Exception as e:
                        self.execution_progress.emit(f"Error aplicando reglas de filtro: {e}")
                        result_df = df
                else:
                    # Compatibilidad: expresión antigua en texto simple
                    filter_expr = config.get('filter_expr')
                    if filter_expr:
                        expr_str = str(filter_expr).strip()
                        try:
                            # Soporte simple para >, <, ==, !=, >=, <=
                            ops = ['>=', '<=', '==', '!=', '>', '<']
                            op = next((o for o in ops if o in expr_str), None)
                            if op:
                                col, val = expr_str.split(op, 1)
                                col = col.strip()
                                val_str = val.strip()
                                try:
                                    val_num = float(val_str)
                                    rhs = val_num
                                except Exception:
                                    rhs = val_str
                                if op == '>':
                                    result_df = df.filter(pl.col(col) > rhs)
                                elif op == '<':
                                    result_df = df.filter(pl.col(col) < rhs)
                                elif op == '==':
                                    result_df = df.filter(pl.col(col) == rhs)
                                elif op == '!=':
                                    result_df = df.filter(pl.col(col) != rhs)
                                elif op == '>=':
                                    result_df = df.filter(pl.col(col) >= rhs)
                                elif op == '<=':
                                    result_df = df.filter(pl.col(col) <= rhs)
                            else:
                                self.execution_progress.emit(f"Expresión de filtro no soportada: {expr_str}")
                                result_df = df
                        except Exception as e:
                            self.execution_progress.emit(f"Error aplicando filtro: {e}")
                            result_df = df
                    else:
                        self.execution_progress.emit(f"No se especificó filtro para nodo {node_id}")
                        result_df = df

            elif subtype == 'join':
                if 'other_dataframe' in config:
                    # Ejecutar join con selección/renombrado consistente
                    return self._execute_join(df, config)
                else:
                    self.execution_progress.emit(f"Faltan parámetros para realizar join en nodo {node_id}")
                    result_df = df

            elif subtype == 'aggregate':
                # Modo nuevo: configuración estructurada
                group_cols_struct = config.get('group_by_list')
                aggs_struct = config.get('aggs')
                if isinstance(group_cols_struct, list) and isinstance(aggs_struct, list):
                    try:
                        agg_exprs = []
                        for agg in aggs_struct:
                            func = str(agg.get('func', '')).lower()
                            col = agg.get('col')
                            alias = agg.get('as') or None
                            if not col:
                                continue
                            expr = None
                            if func == 'sum':
                                expr = pl.sum(col)
                            elif func in ('avg', 'mean'):
                                expr = pl.mean(col)
                            elif func == 'min':
                                expr = pl.min(col)
                            elif func == 'max':
                                expr = pl.max(col)
                            elif func == 'count':
                                expr = pl.count()
                            if expr is not None and alias:
                                expr = expr.alias(alias)
                            if expr is not None:
                                agg_exprs.append(expr)
                        group_cols = [c for c in group_cols_struct if isinstance(c, str) and c]
                        if group_cols and agg_exprs:
                            result_df = df.group_by(group_cols).agg(agg_exprs)
                        else:
                            self.execution_progress.emit(f"Configuración de agregación inválida en nodo {node_id}")
                            result_df = df
                    except Exception as e:
                        self.execution_progress.emit(f"Error en agregación: {e}")
                        result_df = df
                else:
                    # Compatibilidad: strings antiguos
                    if 'group_by' in config and 'agg_funcs' in config:
                        group_cols = [col.strip() for col in str(config['group_by']).split(',') if col.strip()]
                        agg_exprs = []
                        for agg_expr in str(config['agg_funcs']).split(','):
                            if '(' in agg_expr and ')' in agg_expr:
                                func, col = agg_expr.strip().split('(')
                                col = col.replace(')', '').strip()
                                if func.lower() == 'sum':
                                    agg_exprs.append(pl.sum(col))
                                elif func.lower() in ('avg', 'mean'):
                                    agg_exprs.append(pl.mean(col))
                                elif func.lower() == 'min':
                                    agg_exprs.append(pl.min(col))
                                elif func.lower() == 'max':
                                    agg_exprs.append(pl.max(col))
                                elif func.lower() == 'count':
                                    agg_exprs.append(pl.count())
                        if group_cols and agg_exprs:
                            result_df = df.group_by(group_cols).agg(agg_exprs)
                        else:
                            self.execution_progress.emit(f"Expresiones de agregación inválidas en nodo {node_id}")
                            result_df = df
                    else:
                        self.execution_progress.emit(f"Faltan parámetros para agregación en nodo {node_id}")
                        result_df = df

            elif subtype == 'map':
                # Modo nuevo: operaciones estructuradas
                ops = config.get('map_ops')
                if isinstance(ops, list) and ops:
                    try:
                        exprs = []
                        for op in ops:
                            new_col = op.get('new_col')
                            op_type = str(op.get('op_type', '')).lower()
                            a = op.get('a')
                            b = op.get('b')
                            val = op.get('value')
                            expr = None
                            if not new_col:
                                continue
                            if op_type == 'add' and a and b:
                                expr = (pl.col(a) + pl.col(b)).alias(new_col)
                            elif op_type == 'sub' and a and b:
                                expr = (pl.col(a) - pl.col(b)).alias(new_col)
                            elif op_type == 'mul' and a and b:
                                expr = (pl.col(a) * pl.col(b)).alias(new_col)
                            elif op_type == 'div' and a and b:
                                expr = (pl.col(a) / pl.col(b)).alias(new_col)
                            elif op_type == 'concat' and a and b:
                                expr = (pl.col(a).cast(pl.Utf8) + pl.col(b).cast(pl.Utf8)).alias(new_col)
                            elif op_type == 'literal' and val is not None:
                                expr = pl.lit(val).alias(new_col)
                            elif op_type == 'copy' and a:
                                expr = pl.col(a).alias(new_col)
                            elif op_type == 'upper' and a:
                                expr = pl.col(a).cast(pl.Utf8).str.to_uppercase().alias(new_col)
                            elif op_type == 'lower' and a:
                                expr = pl.col(a).cast(pl.Utf8).str.to_lowercase().alias(new_col)
                            elif op_type == 'length' and a:
                                expr = pl.col(a).cast(pl.Utf8).str.len_chars().alias(new_col)
                            if expr is not None:
                                exprs.append(expr)
                        if exprs:
                            result_df = df.with_columns(exprs)
                        else:
                            result_df = df
                    except Exception as e:
                        self.execution_progress.emit(f"Error en mapeo estructurado: {e}")
                        result_df = df
                else:
                    # Compatibilidad: expresión antigua
                    if 'map_expr' in config and config['map_expr']:
                        map_expr = str(config['map_expr']).strip()
                        if '=' in map_expr:
                            new_col, expr = map_expr.split('=', 1)
                            new_col = new_col.strip()
                            expr = expr.strip()
                            if '+' in expr:
                                cols = [c.strip() for c in expr.split('+')]
                                if len(cols) >= 2:
                                    result_df = df.with_columns((pl.col(cols[0]) + pl.col(cols[1])).alias(new_col))
                            elif '-' in expr:
                                cols = [c.strip() for c in expr.split('-')]
                                if len(cols) >= 2:
                                    result_df = df.with_columns((pl.col(cols[0]) - pl.col(cols[1])).alias(new_col))
                            elif '*' in expr:
                                cols = [c.strip() for c in expr.split('*')]
                                if len(cols) >= 2:
                                    result_df = df.with_columns((pl.col(cols[0]) * pl.col(cols[1])).alias(new_col))
                            elif '/' in expr:
                                cols = [c.strip() for c in expr.split('/')]
                                if len(cols) >= 2:
                                    result_df = df.with_columns((pl.col(cols[0]) / pl.col(cols[1])).alias(new_col))
                            else:
                                self.execution_progress.emit(f"Expresión de mapeo no soportada en nodo {node_id}")
                                result_df = df
                        else:
                            self.execution_progress.emit(f"Expresión de mapeo no soportada en nodo {node_id}")
                            result_df = df
                    else:
                        self.execution_progress.emit(f"No se especificó expresión de mapeo para nodo {node_id}")
                        result_df = df
            elif subtype == 'cast':
                # Casteo de tipos de datos
                ops = config.get('cast_ops')
                if isinstance(ops, list) and ops:
                    try:
                        def map_dtype(name: str):
                            n = (name or '').strip().lower()
                            if n == 'int64':
                                return pl.Int64
                            if n == 'int32':
                                return pl.Int32
                            if n == 'float64':
                                return pl.Float64
                            if n == 'float32':
                                return pl.Float32
                            if n in ('utf8', 'string', 'str', 'texto'):
                                return pl.Utf8
                            if n in ('bool', 'boolean'):
                                return pl.Boolean
                            if n == 'date':
                                return pl.Date
                            if n == 'datetime':
                                return pl.Datetime
                            return None
                        exprs = []
                        for op in ops:
                            col = op.get('col')
                            to = op.get('to')
                            dtype = map_dtype(to)
                            if col and dtype is not None and col in result_df.columns:
                                exprs.append(pl.col(col).cast(dtype).alias(col))
                        if exprs:
                            result_df = result_df.with_columns(exprs)
                    except Exception as e:
                        self.execution_progress.emit(f"Error en casteo: {e}")
                        result_df = df
                else:
                    self.execution_progress.emit(f"No se especificaron operaciones de casteo para nodo {node_id}")
                    result_df = df
            else:
                self.execution_progress.emit(f"Tipo de transformación desconocido para nodo {node_id}")
                result_df = df

            # Post-procesamiento: selección y renombrado
            # Nota: los nodos de unión realizan su propia selección/renombrado y ya retornaron.
            result_df = self._apply_select_and_rename(result_df, config)
            try:
                self.execution_progress.emit(f"Nodo {node_id} columnas: {list(result_df.columns)}")
            except Exception:
                pass
            return result_df
        except Exception as e:
            self.execution_progress.emit(f"Error en transformación nodo {node_id}: {e}")
            import traceback
            traceback.print_exc()
            return df
        
    def execute_destination(self, node_id: int, df: pl.DataFrame):
        """Ejecuta un nodo de destino para guardar o enviar el DataFrame."""
        self.execution_progress.emit(f"Ejecutando nodo de destino {node_id}...")
        config = self.pipeline.nodes[node_id]['config']
        subtype = config.get('subtype')

        # Post-procesamiento opcional en destino (selección/renombrado)
        df_to_write = self._apply_select_and_rename(df, config)

        if subtype in ('csv', 'excel', 'json', 'parquet'):
            path = config.get('path')
            if not path:
                self.execution_progress.emit(f"No se especificó ruta de destino para nodo {node_id}")
                raise ValueError(f"No se especificó ruta de destino para nodo {node_id}")
            try:
                if os.path.isdir(path):
                    raise ValueError(f"La ruta especificada es un directorio: {path}")
                out_dir = os.path.dirname(path)
                if out_dir:
                    os.makedirs(out_dir, exist_ok=True)

                # Determinar formato: si hay 'format' úsalo, si no, según subtipo
                default_fmt = 'excel' if subtype == 'excel' else ('json' if subtype == 'json' else ('parquet' if subtype == 'parquet' else 'csv'))
                format_type = (config.get('format') or default_fmt).lower()
                self.execution_progress.emit(f"Guardando datos en {path} como {format_type.upper()}...")

                if format_type == 'csv':
                    df_to_write.write_csv(path)
                elif format_type == 'parquet':
                    df_to_write.write_parquet(path)
                elif format_type == 'json':
                    df_to_write.write_json(path)
                elif format_type == 'excel':
                    # Polars no tiene write_excel estable: usar pandas
                    pdf = df_to_write.to_pandas()
                    try:
                        pdf.to_excel(path, index=False)
                    except Exception as e:
                        raise ValueError(f"Error escribiendo Excel: {e}")
                else:
                    df_to_write.write_csv(path)

                self.execution_progress.emit(f"Datos guardados en {path}")
            except Exception as e:
                self.execution_progress.emit(f"Error al guardar datos: {e}")
                import traceback
                traceback.print_exc()
                raise

        elif subtype == 'database':
            # Escritura a base de datos con pandas + sqlalchemy
            db_type = config.get('db_type')
            host = config.get('host')
            port = config.get('port')
            user = config.get('user')
            password = config.get('password')
            database = config.get('database')
            table = config.get('table')
            if not table:
                raise ValueError("Debe especificar el nombre de la tabla de destino ('table')")
            conn_str = self._build_connection_string(db_type, host, port, user, password, database)
            self.execution_progress.emit(f"Escribiendo datos en base de datos tabla {table}...")
            pdf = df_to_write.to_pandas()
            try:
                if (db_type or '').lower() == 'mysql':
                    engine = self._make_sqlalchemy_engine(db_type, conn_str, config)
                    if_exists = (config.get('if_exists') or 'replace').lower()
                    pdf.to_sql(table, engine, if_exists=if_exists, index=False)
                else:
                    from sqlalchemy import create_engine
                    engine = create_engine(conn_str)
                    if_exists = (config.get('if_exists') or 'replace').lower()
                    pdf.to_sql(table, engine, if_exists=if_exists, index=False)
                self.execution_progress.emit(f"Datos escritos en la tabla {table}")
            except Exception as e:
                # Reintentar alternando SSL (solo MySQL)
                if (db_type or '').lower() == 'mysql' and self._should_retry_ssl(e, config):
                    try:
                        retry_mode = 'DISABLED' if self._was_ssl_enabled(config) else 'REQUIRED'
                        engine = self._make_sqlalchemy_engine(db_type, conn_str, config, ssl_mode_override=retry_mode)
                        if_exists = (config.get('if_exists') or 'replace').lower()
                        pdf.to_sql(table, engine, if_exists=if_exists, index=False)
                        self.execution_progress.emit(f"Reintento MySQL con SSL='{retry_mode}' exitoso")
                    except Exception as ie:
                        self.execution_progress.emit(f"Error al escribir en base de datos: {ie}")
                        import traceback
                        traceback.print_exc()
                        raise
                else:
                    self.execution_progress.emit(f"Error al escribir en base de datos: {e}")
                    import traceback
                    traceback.print_exc()
                    raise

        elif subtype == 'api':
            # Envío de datos a API en JSON (por lotes si es grande)
            url = config.get('url')
            method = (config.get('method') or 'POST').upper()
            headers = self._parse_kv_string(config.get('headers')) if isinstance(config.get('headers'), str) else config.get('headers')
            params = self._parse_kv_string(config.get('params')) if isinstance(config.get('params'), str) else config.get('params')
            if not url:
                raise ValueError("Debe especificar la URL para el destino API")
            self.execution_progress.emit(f"Enviando datos a API {method} {url}...")
            data_records = df_to_write.to_dicts()
            batch_size = int(config.get('batch_size') or 500)
            try:
                for i in range(0, len(data_records), batch_size):
                    if self._stop_requested:
                        raise KeyboardInterrupt("Ejecución detenida por el usuario")
                    batch = data_records[i:i+batch_size]
                    resp = requests.request(method, url, headers=headers, params=params, json=batch, timeout=60)
                    if not resp.ok:
                        raise ValueError(f"Error de API (status {resp.status_code}): {resp.text}")
                    self.execution_progress.emit(f"Lote {i//batch_size + 1} enviado ({len(batch)} registros)")
                self.execution_progress.emit("Envío a API completado")
            except Exception as e:
                self.execution_progress.emit(f"Error al enviar a API: {e}")
                raise

        else:
            self.execution_progress.emit(f"Tipo de destino desconocido para nodo {node_id}")
            
    def execute_pipeline(self, node_configs=None):
        """Execute the entire pipeline"""
        try:
            self.execution_progress.emit("Iniciando ejecución del pipeline...")
            self._stop_requested = False
            
            # Validar pipeline
            if not nx.is_directed_acyclic_graph(self.pipeline):
                self.execution_progress.emit("Error: El pipeline contiene ciclos")
                self.execution_finished.emit(False, "El pipeline contiene ciclos")
                return False
                
            # Si se proporcionaron configuraciones, actualizarlas
            if node_configs:
                for node_id, config in node_configs.items():
                    if node_id in self.pipeline.nodes:
                        self.pipeline.nodes[node_id]['config'] = config
                
            # Get topological sort of nodes
            sorted_nodes = list(nx.topological_sort(self.pipeline))
            
            # Verificar que hay al menos un nodo fuente
            has_source = False
            for node_id in sorted_nodes:
                if self.pipeline.nodes[node_id]['type'] == 'source':
                    has_source = True
                    break
                    
            if not has_source:
                self.execution_progress.emit("Error: El pipeline no tiene nodos de origen")
                self.execution_finished.emit(False, "El pipeline no tiene nodos de origen")
                return False
                
            # Execute pipeline
            node_results = {}
            for node_id in sorted_nodes:
                if self._stop_requested:
                    self.execution_progress.emit("Ejecución detenida por el usuario")
                    self.execution_finished.emit(False, "Ejecución detenida por el usuario")
                    return False
                node_type = self.pipeline.nodes[node_id]['type']
                
                try:
                    if node_type == 'source':
                        df = self.execute_source(node_id)
                        node_results[node_id] = df
                        self.node_dataframes[node_id] = df
                        self.node_executed.emit(node_id, df)
                        
                    elif node_type == 'transform':
                        # Obtener dataframes de entrada
                        input_dfs = []
                        for pred in self.pipeline.predecessors(node_id):
                            if pred in node_results:
                                input_dfs.append(node_results[pred])
                                
                        if not input_dfs:
                            self.execution_progress.emit(f"Error: Nodo {node_id} no tiene entradas")
                            continue
                            
                        # Para simplificar, usamos solo el primer dataframe como entrada principal
                        input_df = input_dfs[0]
                        
                        # Si es un nodo de unión y hay más de una entrada, preparar lado derecho
                        config = self.pipeline.nodes[node_id]['config']
                        if config.get('subtype') == 'join' and len(input_dfs) > 1:
                            # Respetar bandera de intercambio de entradas (swap_inputs)
                            try:
                                if bool(config.get('swap_inputs')):
                                    input_df, input_dfs[1] = input_dfs[1], input_df
                            except Exception:
                                pass
                            config['other_dataframe'] = input_dfs[1]
                            
                        # Ejecutar transformación
                        result_df = self.execute_transform(node_id, input_df)
                        
                        # Asegurarse que el resultado sea válido
                        if result_df is not None:
                            node_results[node_id] = result_df
                            self.node_dataframes[node_id] = result_df
                            self.node_executed.emit(node_id, result_df)
                        
                    elif node_type == 'destination':
                        # Obtener dataframe de entrada
                        preds = list(self.pipeline.predecessors(node_id))
                        if not preds or preds[0] not in node_results:
                            self.execution_progress.emit(f"Error: Nodo destino {node_id} no tiene entrada válida")
                            continue
                            
                        # Asegurar que usamos el dataframe actualizado más reciente
                        source_id = preds[0]
                        input_df = node_results[source_id]
                        
                        # Actualizamos la configuración del nodo destino con el dataframe actualizado
                        self.pipeline.nodes[node_id]['config']['dataframe'] = input_df
                        
                        # Ejecutar el nodo destino con el dataframe actualizado
                        self.execute_destination(node_id, input_df)
                        
                        # Guardar el dataframe en el nodo destino también
                        self.node_dataframes[node_id] = input_df
                        self.node_executed.emit(node_id, input_df)
                        
                except Exception as e:
                    self.execution_progress.emit(f"Error en nodo {node_id}: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    self.execution_finished.emit(False, f"Error en nodo {node_id}: {str(e)}")
                    return False
                    
            self.execution_progress.emit("Pipeline ejecutado correctamente")
            self.execution_finished.emit(True, "Pipeline ejecutado correctamente")
            return node_results
            
        except Exception as e:
            self.execution_progress.emit(f"Error al ejecutar pipeline: {str(e)}")
            import traceback
            traceback.print_exc()
            self.execution_finished.emit(False, f"Error al ejecutar pipeline: {str(e)}")
            return False

    # Utilidades
    def request_stop(self):
        """Solicita detener la ejecución del pipeline lo más pronto posible."""
        self._stop_requested = True

    def _parse_kv_string(self, s: Optional[str]) -> Optional[Dict[str, str]]:
        """Convierte un string tipo 'k1:v1,k2:v2' en dict. Ignora entradas malformadas."""
        if s is None:
            return None
        if isinstance(s, dict):
            return s  # ya es dict
        result: Dict[str, str] = {}
        try:
            pairs = [p for p in str(s).split(',') if p.strip()]
            for p in pairs:
                if ':' in p:
                    k, v = p.split(':', 1)
                    result[k.strip()] = v.strip()
        except Exception:
            pass
        return result

    def _execute_filter_rules(self, df: pl.DataFrame, rules: list, mode: str) -> pl.DataFrame:
        """Aplica reglas de filtro estructuradas.
        Cada regla: {column, op, value}
        op en: '>', '<', '==', '!=', '>=', '<=', 'contains', 'in', 'isnull', 'notnull'
        mode: 'all' (AND) o 'any' (OR)
        """
        exprs = []
        for r in rules:
            col = r.get('column')
            op = str(r.get('op', '')).lower()
            val = r.get('value')
            if not col or not op:
                continue
            e = None
            if op == '>':
                e = pl.col(col) > val
            elif op == '<':
                e = pl.col(col) < val
            elif op == '==':
                e = pl.col(col) == val
            elif op == '!=':
                e = pl.col(col) != val
            elif op == '>=':
                e = pl.col(col) >= val
            elif op == '<=':
                e = pl.col(col) <= val
            elif op == 'contains' and isinstance(val, str):
                e = pl.col(col).cast(pl.Utf8).str.contains(val)
            elif op == 'in':
                try:
                    seq = list(val) if not isinstance(val, list) else val
                except Exception:
                    seq = [val]
                e = pl.col(col).is_in(seq)
            elif op == 'isnull':
                e = pl.col(col).is_null()
            elif op == 'notnull':
                e = pl.col(col).is_not_null()
            if e is not None:
                exprs.append(e)
        if not exprs:
            return df
        final = exprs[0]
        for e in exprs[1:]:
            if mode == 'any':
                final = final | e
            else:
                final = final & e
        return df.filter(final)

    def _execute_join(self, left_df: pl.DataFrame, config: Dict[str, Any]) -> pl.DataFrame:
        """Ejecuta un join entre left_df y config['other_dataframe'] respetando join_cols/join_pairs,
        tipo de join, sufijo derecho y aplica selección/renombrado con nombres calificados.
        """
        try:
            right_df = config.get('other_dataframe')
            if isinstance(right_df, pd.DataFrame):
                right_df = pl.from_pandas(right_df)
            if isinstance(left_df, pd.DataFrame):
                left_df = pl.from_pandas(left_df)

            join_type = (config.get('join_type', 'Inner') or 'Inner').lower()
            right_suffix = str(config.get('right_suffix') or '_right')

            # Soporte de pares (left:right) o un listado simple 'on'
            left_on: List[str] = []
            right_on: List[str] = []
            join_pairs = str(config.get('join_pairs') or '').strip()
            if join_pairs:
                for p in [s for s in join_pairs.split(',') if s.strip()]:
                    if ':' in p:
                        l, r = p.split(':', 1)
                        left_on.append(l.strip())
                        right_on.append(r.strip())
            if not left_on:
                # Fallback a join_cols para ambos lados
                base_cols = [c.strip() for c in str(config.get('join_cols') or '').split(',') if c.strip()]
                left_on = base_cols
                right_on = base_cols

            # Ejecutar join
            how = join_type if join_type in ('inner', 'left', 'right', 'outer') else 'inner'
            result = left_df.join(right_df, left_on=left_on, right_on=right_on, how=how, suffix=right_suffix)

            # Construir selección/renombrado respetando nombres calificados Origen1./Origen2.
            left_cols = list(left_df.columns)
            selections_full: List[str] = []
            out_spec = config.get('output_cols')
            if isinstance(out_spec, str) and out_spec.strip():
                selections_full = [c.strip() for c in out_spec.split(',') if c.strip()]
            rename_full: Dict[str, str] = {}
            rn_spec = config.get('column_rename')
            if isinstance(rn_spec, str) and rn_spec.strip():
                for pair in rn_spec.split(','):
                    if ':' in pair:
                        k, v = pair.split(':', 1)
                        rename_full[k.strip()] = v.strip()

            def map_qualified_to_actual(qname: str) -> Optional[str]:
                name = qname.strip()
                src = None
                base = name
                if name.startswith('Origen1.'):
                    src = 1
                    base = name.split('.', 1)[1]
                elif name.startswith('Origen2.'):
                    src = 2
                    base = name.split('.', 1)[1]
                if src == 1:
                    return base if base in result.columns else base
                elif src == 2:
                    if base in left_cols and base not in left_on:
                        cand = f"{base}{right_suffix}"
                        return cand if cand in result.columns else cand
                    else:
                        return base
                else:
                    return base

            final_columns: List[str] = []
            if selections_full:
                for q in selections_full:
                    actual = map_qualified_to_actual(q)
                    if actual and actual not in final_columns:
                        final_columns.append(actual)
            else:
                final_columns = list(result.columns)

            rename_actual: Dict[str, str] = {}
            for qkey, new_name in rename_full.items():
                actual = map_qualified_to_actual(qkey)
                if actual and new_name:
                    rename_actual[actual] = new_name
            for qkey, new_name in rename_full.items():
                if '.' not in qkey and qkey in final_columns and qkey not in rename_actual:
                    rename_actual[qkey] = new_name
                elif '.' not in qkey and f"{qkey}{right_suffix}" in final_columns and f"{qkey}{right_suffix}" not in rename_actual:
                    rename_actual[f"{qkey}{right_suffix}"] = new_name

            if final_columns:
                try:
                    result = result.select([pl.col(c) for c in final_columns])
                except Exception:
                    pass
            if rename_actual:
                try:
                    result = result.rename(rename_actual)
                except Exception:
                    pass
            return result
        except Exception as e:
            self.execution_progress.emit(f"Error ejecutando join: {e}")
            import traceback
            traceback.print_exc()
            return left_df

    def _apply_select_and_rename(self, df: pl.DataFrame, config: Dict[str, Any]) -> pl.DataFrame:
        """Aplica selección y renombrado de columnas según 'output_cols' y 'column_rename'."""
        result = df
        try:
            # Selección de columnas
            output_cols = config.get('output_cols')
            if output_cols and isinstance(output_cols, str):
                cols = [c.strip() for c in output_cols.split(',') if c.strip()]
                # Remover prefijos tipo OrigenX.
                processed_cols = [c.split('.', 1)[1] if '.' in c else c for c in cols]
                valid_cols = [c for c in processed_cols if c in result.columns]
                if valid_cols:
                    result = result.select(valid_cols)

            # Renombrado de columnas
            rename_spec = config.get('column_rename')
            if rename_spec and isinstance(rename_spec, str):
                rename_pairs = rename_spec.split(',')
                rename_dict = {}
                for pair in rename_pairs:
                    if ':' in pair:
                        old_name, new_name = pair.split(':', 1)
                        old_name = old_name.strip()
                        new_name = new_name.strip()
                        if '.' in old_name:
                            old_name = old_name.split('.', 1)[1]
                        if old_name in result.columns and new_name:
                            rename_dict[old_name] = new_name
                if rename_dict:
                    result = result.rename(rename_dict)
        except Exception as e:
            self.execution_progress.emit(f"Aviso: error aplicando selección/renombrado: {e}")
        return result

    def _build_connection_string(self, db_type: Optional[str], host: Optional[str], port: Optional[str], user: Optional[str], password: Optional[str], database: Optional[str]) -> str:
        db_type = (db_type or '').lower()
        if db_type == 'sqlite':
            # Para SQLite el 'host' se usa como path del archivo si viene en 'database'
            if database and (database.endswith('.db') or database.endswith('.sqlite3')):
                return f"sqlite:///{database}"
            elif host and (host.endswith('.db') or host.endswith('.sqlite3')):
                return f"sqlite:///{host}"
            else:
                raise ValueError("Para SQLite especifique el path del archivo en 'database' o 'host'")
        elif db_type in ('postgresql', 'postgres'):
            return f"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}"
        elif db_type == 'mysql':
            return f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}"
        elif db_type in ('mssql', 'sql server', 'sqlserver'):
            # Requiere mssql+pyodbc con DSN apropiado; se asume driver por defecto
            return f"mssql+pyodbc://{user}:{password}@{host}:{port}/{database}?driver=ODBC+Driver+17+for+SQL+Server"
        else:
            raise ValueError(f"Tipo de base de datos no soportado: {db_type}")

    def _read_sql(self, conn_str: str, query: str) -> pl.DataFrame:
        """Lee datos SQL intentando con connectorx y haciendo fallback a pandas+sqlalchemy."""
        try:
            import connectorx as cx
            pdf = cx.read_sql(conn_str, query)
            # connectorx puede devolver pandas DataFrame
            if isinstance(pdf, pd.DataFrame):
                return pl.from_pandas(pdf)
            # Si devuelve un PyArrow Table
            try:
                import pyarrow as pa
                if isinstance(pdf, pa.Table):
                    return pl.from_arrow(pdf)
            except Exception:
                pass
            # Intentar crear polars directamente si fuese soportado
            return pl.DataFrame(pdf)
        except Exception:
            from sqlalchemy import create_engine
            engine = create_engine(conn_str)
            pdf = pd.read_sql_query(query, engine)
            return pl.from_pandas(pdf)

    def _make_sqlalchemy_engine(self, db_type: Optional[str], conn_str: str, config: Dict[str, Any], ssl_mode_override: Optional[str] = None):
        """Crea un engine SQLAlchemy contemplando SSL/timeout para MySQL.
        Config soportada en nodos DB (MySQL):
          - ssl_mode: 'DISABLED' | 'REQUIRED' | 'VERIFY_CA' | 'VERIFY_IDENTITY'
          - ssl_ca, ssl_cert, ssl_key, ssl_verify (bool)
          - connect_timeout (segundos)
        """
        from sqlalchemy import create_engine
        connect_args: Dict[str, Any] = {}
        dbt = (db_type or '').lower()
        if dbt == 'mysql':
            # Timeout de conexión
            try:
                timeout = int(config.get('connect_timeout') or 10)
                connect_args['connect_timeout'] = timeout
            except Exception:
                connect_args['connect_timeout'] = 10
            # SSL
            mode = (ssl_mode_override or config.get('ssl_mode') or '').strip().upper()
            if mode in ('REQUIRED', 'VERIFY_CA', 'VERIFY_IDENTITY'):
                import ssl as _ssl
                ssl_dict: Dict[str, Any] = {}
                ca = config.get('ssl_ca')
                cert = config.get('ssl_cert')
                key = config.get('ssl_key')
                verify = str(config.get('ssl_verify') or '').strip().lower() in ('1', 'true', 'yes')
                if not verify:
                    ssl_dict['cert_reqs'] = _ssl.CERT_NONE
                if ca:
                    ssl_dict['ca'] = ca
                if cert:
                    ssl_dict['cert'] = cert
                if key:
                    ssl_dict['key'] = key
                connect_args['ssl'] = ssl_dict
        # Otros motores: sin cambios
        return create_engine(conn_str, connect_args=connect_args, pool_pre_ping=True, pool_recycle=300)

    def _was_ssl_enabled(self, config: Dict[str, Any]) -> bool:
        mode = str(config.get('ssl_mode') or '').strip().upper()
        return mode in ('REQUIRED', 'VERIFY_CA', 'VERIFY_IDENTITY')

    def _should_retry_ssl(self, exc: Exception, config: Dict[str, Any]) -> bool:
        """Heurística para alternar SSL en errores MySQL.
        Retorna True si el mensaje sugiere problem de SSL o timeout (para probar el modo opuesto).
        """
        try:
            msg = str(exc).lower()
        except Exception:
            return False
        if any(k in msg for k in ['ssl', 'handshake', 'certificate', 'tls']):
            return True
        if 'timed out' in msg or 'timeout' in msg:
            return True
        return False